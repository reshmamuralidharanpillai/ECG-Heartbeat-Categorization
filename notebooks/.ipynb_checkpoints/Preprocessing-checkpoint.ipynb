{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import get_train_data,get_train_split,get_test_split,preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the feature and target variables from the train and test datasets respectively\n",
    "X_train,y_train =  get_train_split()\n",
    "X_test,y_test = get_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Choosing a data augmentation technique to enhance the diversity of the dataset should be done keeping in mind the nature of the dataset and the domain. In addition to general data augmentation techniques, there are several domain-specific data augmentation techniques that can be used in healthcare datasets. These techniques take into account the specific characteristics and requirements of medical data.\n",
    "\n",
    "To balance the class distribution, either over-sampling or under-sampling could be done. Downsampling can be aceived using **RandomUnderSampler**. Since, downsampling can lead to loass of information, upsampling was followed.\n",
    "This project makes use of **SMOTE** in addressing the class imbalance. SMOTE is specifically designed to address class imbalance by generating synthetic samples for the minority class, in this case, the underrepresented abnormal heartbeats. By increasing the number of synthetic minority class samples, SMOTE helps balance the class distribution, leading to improved model performance. It helps prevent the model from being overly biased towards the majority class. With more diverse samples, the model can learn to identify patterns and variations associated with abnormal heartbeats, improving its ability to correctly classify unseen instances during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "**Principal Component Analysis** is commonly used to reduce the dimensionality of high-dimensional data. In a heartbeat classification task, by applying PCA, the dimensionality of the feature space has be reduced, while still retaining the most important information present in the data. This helps in reducing computational complexity and improving model efficiency. \n",
    "\n",
    "PCA has been used to retain the principal components which account for 95% of variance in data. As a result,the dimensionality of the dataset has reduced to (362355, 29). The reduced-dimensional feature space obtained through PCA can help improve model performance in heartbeat classification tasks. By selecting the most informative principal components, the model can focus on the essential features, reducing the risk of overfitting and improving generalization.  \n",
    "\n",
    "It is essential to *standardize* the data before performing PCA to ensure that all the data have *zero mean and unit variance*. Since the dataset at hand is already standardized(checked using StandardScaler), did not have to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 72471, 4.0: 6431, 2.0: 5788, 1.0: 2223, 3.0: 641})\n",
      "Counter({0.0: 72471, 1.0: 72471, 2.0: 72471, 3.0: 72471, 4.0: 72471})\n",
      "\n",
      "\n",
      "The features have zero mean and unit variance.\n",
      "\n",
      "\n",
      "Explained variance ratio: [0.43600339 0.12020286 0.08053981 0.04585722 0.03819538 0.03286634\n",
      " 0.02462946 0.02214805 0.01758784 0.01574594 0.01391617 0.0120935\n",
      " 0.01020785 0.00898517 0.0081478  0.00747258 0.00695072 0.00645686\n",
      " 0.00591228 0.00544051 0.00487866 0.00437789 0.00424439 0.00374857\n",
      " 0.00356697 0.00343599 0.00323012 0.00298821 0.00287535]\n",
      "\n",
      "\n",
      "Principal components: [[-0.09055349 -0.07082363 -0.02486204 ...  0.0046117   0.00445436\n",
      "   0.00436019]\n",
      " [ 0.08265825  0.05845756  0.06012891 ...  0.01997771  0.01910435\n",
      "   0.01863422]\n",
      " [-0.08799919 -0.05715752 -0.12904523 ...  0.03641012  0.03497725\n",
      "   0.03442125]\n",
      " ...\n",
      " [-0.10113452  0.00867999  0.13066281 ... -0.03729483 -0.03680375\n",
      "  -0.03586752]\n",
      " [-0.01808309  0.03983699  0.02062407 ...  0.05903707  0.05806673\n",
      "   0.05685286]\n",
      " [-0.12622701  0.03360272  0.13801579 ... -0.04903527 -0.04878096\n",
      "  -0.04773066]]\n",
      "\n",
      "\n",
      "Transformed data shape: (362355, 29)\n",
      "\n",
      "\n",
      "[0.43600339 0.55620625 0.63674606 0.68260328 0.72079866 0.753665\n",
      " 0.77829446 0.80044251 0.81803036 0.8337763  0.84769246 0.85978596\n",
      " 0.86999381 0.87897898 0.88712678 0.89459937 0.90155009 0.90800694\n",
      " 0.91391922 0.91935973 0.9242384  0.92861629 0.93286068 0.93660925\n",
      " 0.94017622 0.94361221 0.94684233 0.94983054 0.95270589]\n"
     ]
    }
   ],
   "source": [
    "#Passing the X and y to get the preprocessed(after SMOTE and PCA) versions.\n",
    "X_train_processed, y_train_processed, X_test_ = preprocess(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "183    0\n",
       "184    0\n",
       "185    0\n",
       "186    0\n",
       "187    0\n",
       "Length: 188, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check missing or NaN values in the dataset\n",
    "ecg_train = get_train_data()\n",
    "ecg_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset.If there are very minimal number of missing values, the best practice is to drop them. If not, the domain specific data imputations could be used. The simplest way of imputing missing values is using SimpleImputer from scikit-learn. It handles missing values by filling them with a constant value, the mean, median, or most frequent value of the available data. Since the healthcare data is sensitive, some domain specific methods can be used based on the characteristics of the data, the nature of missingness, and the intended analysis. Some common methods can include Clinical Expert Imputation, Deep Learning-based Imputation, Pattern Recognition and Time-Series Analysis, Patient Similarity-based Imputation, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
